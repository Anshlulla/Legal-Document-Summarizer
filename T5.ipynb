{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d72cf35-79e7-4273-a18b-ab9bf0767d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import bitsandbytes as bnb\n",
    "import random\n",
    "import string\n",
    "import inflect\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "#from bert_score import BERTScorer\n",
    "from rouge import Rouge\n",
    "from transformers import Trainer, TrainingArguments, AdamW, T5ForConditionalGeneration, T5Tokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "import pickle\n",
    "import sys\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm.notebook import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aef028c-2323-4766-813a-274a1887be71",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.setrecursionlimit(4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9234ab5-f2ce-4091-8c2a-41e106d1353d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd107b20-af89-4fb8-8cf1-6ec7a6a13c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d11b13c3-58d4-45f0-a749-b8068593bd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76e40d8d-0cee-4972-9a82-de77c209369d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jelly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\jelly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jelly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jelly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\jelly\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76336963-9966-4f97-9e75-c4bc347eab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = word_tokenize(text)\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Initialize inflect engine\n",
    "p = inflect.engine()\n",
    "\n",
    "# Convert numbers to words with error handling for large numbers\n",
    "def convert_numbers_in_text(text):\n",
    "    words = text.split()\n",
    "    new_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            try:\n",
    "                word_in_words = p.number_to_words(word)\n",
    "            except Exception:  # Catch any exception, including out-of-range numbers\n",
    "                word_in_words = word  # Leave the number unchanged if there's an error\n",
    "            new_words.append(word_in_words)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    \n",
    "    return ' '.join(new_words)\n",
    "\n",
    "# Main preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = lowercase_text(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatize_text(text)\n",
    "    text = convert_numbers_in_text(text)  # Combined the number conversion here\n",
    "    return text\n",
    "\n",
    "# Access and preprocess text from document and summary files\n",
    "def access_and_preprocess_text(doc_file_path, summary_file_path):\n",
    "    with open(doc_file_path, 'r', encoding='utf-8') as doc_file:\n",
    "        doc_text = doc_file.read()\n",
    "        processed_doc_text = preprocess_text(doc_text)\n",
    "\n",
    "    with open(summary_file_path, 'r', encoding='utf-8') as summary_file:\n",
    "        summary_text = summary_file.read()\n",
    "        processed_summary_text = preprocess_text(summary_text)\n",
    "\n",
    "    return processed_doc_text, processed_summary_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8eb6b105-40a1-40ed-9b46-3383901abca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists for training documents (X_train) and summaries (y_train)\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "# Initialize empty lists for testing documents (X_test) and summaries (y_test)\n",
    "X_test = []\n",
    "y_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e095df4-bb07-42e1-a57d-ae75c87f3ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories for training and testing documents and summaries\n",
    "train_doc_folder = r\"C:/Users/jelly/Desktop/Sem-4/SML/Research Paper/train_data/judgement\"\n",
    "train_summary_folder = r\"C:/Users/jelly/Desktop/Sem-4/SML/Research Paper/train_data/summary\"\n",
    "test_doc_folder = r\"C:/Users/jelly/Desktop/Sem-4/SML/Research Paper/test_data/judgement\"\n",
    "test_summary_folder = r\"C:/Users/jelly/Desktop/Sem-4/SML/Research Paper/test_data/summary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96f014c9-32d0-42e6-9196-08d3770d4ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the number of training and testing documents to process\n",
    "#num_train_documents = 7030\n",
    "#num_test_documents = 100\n",
    "\n",
    "num_train_documents = 500\n",
    "num_test_documents = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53a549da-fcc9-4f6f-a4ce-f65eefe5917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of training and testing document files\n",
    "train_doc_files = os.listdir(train_doc_folder)\n",
    "train_summary_files = os.listdir(train_summary_folder)\n",
    "test_doc_files = os.listdir(test_doc_folder)\n",
    "test_summary_files = os.listdir(test_summary_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea184357-10f8-4e28-9afd-98c73e59b176",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Process the specified number of training documents and collect X_train and y_train\n",
    "for i in range(num_train_documents):\n",
    "    doc_file = train_doc_files[i]\n",
    "    summary_file = train_summary_files[i]\n",
    "    processed_doc_text, processed_summary_text = access_and_preprocess_text(os.path.join(train_doc_folder, doc_file), os.path.join(train_summary_folder, summary_file))\n",
    "    X_train.append(processed_doc_text)\n",
    "    y_train.append(processed_summary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c3e23f1-4d03-475e-9ca2-a799d180bb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the specified number of testing documents and collect X_test and y_test\n",
    "for i in range(num_test_documents):\n",
    "    doc_file = test_doc_files[i]\n",
    "    summary_file = test_summary_files[i]\n",
    "    processed_doc_text, processed_summary_text = access_and_preprocess_text(os.path.join(test_doc_folder, doc_file), os.path.join(test_summary_folder, summary_file))\n",
    "    X_test.append(processed_doc_text)\n",
    "    y_test.append(processed_summary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5443be5-5ece-4259-991a-8b7d3269ff4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up validation dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7a557d3-1ba2-4410-a177-c6ee6bf364ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Documents: 375\n",
      "Total Validation Documents: 125\n",
      "Total Testing Documents: 100\n",
      "Total Training Summaries: 375\n",
      "Total Validation Summaries: 125\n",
      "Total Testing Summaries: 100\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Training Documents: {len(X_train)}\")\n",
    "print(f\"Total Validation Documents: {len(X_val)}\")\n",
    "print(f\"Total Testing Documents: {len(X_test)}\")\n",
    "print(f\"Total Training Summaries: {len(y_train)}\")\n",
    "print(f\"Total Validation Summaries: {len(y_val)}\")\n",
    "print(f\"Total Testing Summaries: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94bbef82-d208-4c61-8ba0-04a082c09b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the TF-IDF vectorizer with updated parameters\n",
    "tfidf = TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1, 2))\n",
    "\n",
    "# Vectorize the training, validation, and testing data\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf.transform(X_val)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "y_train_tfidf = tfidf.fit_transform(y_train)\n",
    "y_val_tfidf = tfidf.transform(y_val)\n",
    "y_test_tfidf = tfidf.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9d0d492-9b1b-4dcf-a09c-bc4754abf019",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Load the BART model and tokenizer\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "\n",
    "# Load the T5 model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-large')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
    "\"\"\"\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "# Load the DistilBART model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-large')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
    "\n",
    "# Move model to GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Enable gradient checkpointing to save memory\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd65fd63-5313-4aea-8ae6-b575a3f9f76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NaN\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.data.isnan().any():\n",
    "        print(f\"NaNs detected in parameter: {name}\")\n",
    "else:\n",
    "    print(\"No NaN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d33e98d4-986a-4e3e-9999-8ce7642d9fda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NaN\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None and param.grad.isnan().any():\n",
    "        print(f\"NaNs detected in gradient: {name}\")\n",
    "else:\n",
    "    print(\"No NaN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d63be914-8293-4572-80e8-b3494f43a5ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61be0c95-748b-468b-8b00-1253d1b2b494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated memory: 2814.48 MB\n",
      "Reserved memory: 2816.00 MB\n",
      "Free memory: 5371.50 MB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "allocated_memory = torch.cuda.memory_allocated()\n",
    "# Get the current GPU memory reserved (including overhead)\n",
    "reserved_memory = torch.cuda.memory_reserved()\n",
    "\n",
    "# Print memory in MB\n",
    "print(f\"Allocated memory: {allocated_memory / (1024 ** 2):.2f} MB\")\n",
    "print(f\"Reserved memory: {reserved_memory / (1024 ** 2):.2f} MB\")\n",
    "\n",
    "# Get the free memory (total - reserved memory)\n",
    "free_memory = torch.cuda.get_device_properties(0).total_memory - reserved_memory\n",
    "print(f\"Free memory: {free_memory / (1024 ** 2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ffd3395-6ad4-42ec-b2f3-8873e58c128f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clear GPU memory\n",
    "def clear_gpu_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    gc.collect()# Ensure all GPU operations are finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d49a01b6-88cb-400d-95a8-f06f8a641833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9499c47897f4d50a271993667e4c944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/3:   0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 126.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 7.12 GiB is allocated by PyTorch, and 77.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(loss) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(loss):\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN or Inf detected in loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Perform optimizer step every few steps\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    523\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[0;32m    290\u001b[0m     tensors,\n\u001b[0;32m    291\u001b[0m     grad_tensors_,\n\u001b[0;32m    292\u001b[0m     retain_graph,\n\u001b[0;32m    293\u001b[0m     create_graph,\n\u001b[0;32m    294\u001b[0m     inputs,\n\u001b[0;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    297\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\torch\\autograd\\graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    769\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    770\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 126.00 MiB. GPU 0 has a total capacity of 8.00 GiB of which 0 bytes is free. Of the allocated memory 7.12 GiB is allocated by PyTorch, and 77.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "def preprocess_for_training(doc_texts, summaries):\n",
    "    inputs = tokenizer(doc_texts, max_length=512, truncation=True, padding='max_length', return_tensors='pt', return_attention_mask=True)\n",
    "    labels = tokenizer(summaries, max_length=150, truncation=True, padding='max_length', return_tensors='pt').input_ids\n",
    "\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    return inputs, labels\n",
    "\n",
    "# Convert data to appropriate format\n",
    "train_encodings, train_labels = preprocess_for_training(X_train, y_train)\n",
    "val_encodings, val_labels = preprocess_for_training(X_val, y_val)\n",
    "\n",
    "# Dataset class\n",
    "class SummaryDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SummaryDataset(train_encodings, train_labels)\n",
    "val_dataset = SummaryDataset(val_encodings, val_labels)\n",
    "\n",
    "# Create DataLoaders with batch_size=1 for reduced memory usage\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Define gradient accumulation steps to simulate larger batch sizes\n",
    "accumulation_steps = 2  # Adjust this as needed based on memory\n",
    "\n",
    "# Training loop with tqdm\n",
    "num_epochs = 3\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()  # Reset gradients at the start of the epoch\n",
    "    progress_bar = notebook_tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', leave=False)\n",
    "\n",
    "    for i, batch in enumerate(progress_bar):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss / accumulation_steps  # Normalize loss for gradient accumulation\n",
    "\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            raise ValueError(f\"NaN or Inf detected in loss: {loss.item()}\")\n",
    "            \n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimizer step every few steps\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            torch.cuda.synchronize()  # Ensure operations complete before moving forward\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()  # Clear gradients after step\n",
    "\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        del input_ids\n",
    "        del attention_mask\n",
    "        del labels\n",
    "        del outputs\n",
    "        del loss\n",
    "\n",
    "        # Clear GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Print GPU memory usage to monitor consumption\n",
    "    print(f\"Allocated memory: {torch.cuda.memory_allocated() / 1024 ** 2:.2f} MB\")\n",
    "    print(f\"Reserved memory: {torch.cuda.memory_reserved() / 1024 ** 2:.2f} MB\")\n",
    "\n",
    "    print(f'Epoch {epoch + 1} completed')\n",
    "\n",
    "\n",
    "# Save the fine-tuned T5 model and tokenizer\n",
    "model.save_pretrained('t5-finetuned')\n",
    "tokenizer.save_pretrained('t5-finetuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ee462d-0514-41c9-afeb-82ab08b69713",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "allocated_memory = torch.cuda.memory_allocated()\n",
    "# Get the current GPU memory reserved (including overhead)\n",
    "reserved_memory = torch.cuda.memory_reserved()\n",
    "\n",
    "# Print memory in MB\n",
    "print(f\"Allocated memory: {allocated_memory / (1024 ** 2):.2f} MB\")\n",
    "print(f\"Reserved memory: {reserved_memory / (1024 ** 2):.2f} MB\")\n",
    "\n",
    "# Get the free memory (total - reserved memory)\n",
    "free_memory = torch.cuda.get_device_properties(0).total_memory - reserved_memory\n",
    "print(f\"Free memory: {free_memory / (1024 ** 2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a343997b-bb9e-4971-90ba-4328078c9346",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"train_evaluation.csv\"\n",
    "\n",
    "# Open the CSV file and write the headers\n",
    "with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"document\", \"actual_summary\", \"predicted_summary\", \"cosine_similarity\", \"rouge_1\", \"rouge_2\", \"rouge_L\"])  # Add headers\n",
    "    \n",
    "    # Iterate over 30 documents\n",
    "    for i in tqdm(range(30), desc=\"Processing Documents\", leave=False):\n",
    "        print(f\"\\n### Document: {i+1} ###\\n\")\n",
    "        \n",
    "        # Generate summary for the document\n",
    "        summary = summarize_chunks(X_train[i])\n",
    "        \n",
    "        # Compute cosine similarity using TF-IDF vectors (ensure `tfidf` is already defined and fitted)\n",
    "        cosine_sim = cosine_similarity(tfidf.transform([y_train[i]]), tfidf.transform([summary]))[0][0]\n",
    "        \n",
    "        # Compute ROUGE scores (assuming rouge_scores is a predefined function)\n",
    "        rouge_1, rouge_2, rouge_L = rouge_scores(y_train[i], summary)\n",
    "        \n",
    "        # Print metrics for inspection\n",
    "        print(f\"\\nCosine similarity: {cosine_sim}\")\n",
    "        print(f\"ROUGE-1: {rouge_1}\")\n",
    "        print(f\"ROUGE-2: {rouge_2}\")\n",
    "        print(f\"ROUGE-L: {rouge_L}\\n\")\n",
    "        \n",
    "        # Write the data to CSV file\n",
    "        writer.writerow([X_train[i], y_train[i], summary, cosine_sim, rouge_1, rouge_2, rouge_L])\n",
    "        \n",
    "        # Clear memory for unused variables\n",
    "        summary = summary.cpu()\n",
    "        del summary\n",
    "        del cosine_sim\n",
    "        del rouge_1\n",
    "        del rouge_2\n",
    "        del rouge_L\n",
    "        \n",
    "        # Explicitly clear GPU memory and trigger garbage collection\n",
    "        clear_gpu_memory()\n",
    "\n",
    "print(\"Processing and evaluation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eeac20-5fbd-4e13-8778-a018f2ff090f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_scores(actual_summary, predicted_summary):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(predicted_summary, actual_summary)\n",
    "    return scores[0]['rouge-1']['f'], scores[0]['rouge-2']['f'], scores[0]['rouge-l']['f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c33f2f3-5f5c-4f65-914e-49f8d8f58f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clear GPU memory\n",
    "def clear_gpu_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    gc.collect()# Ensure all GPU operations are finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34013da6-9394-4924-9eaf-001839202c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf612cf-575e-4a7a-97a5-ee763d1bd145",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"train_evaluation.csv\"\n",
    "with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"document\", \"actual_summary\", \"predicted_summary\", \"cosine_similarity\", \"rouge_1\", \"rouge_2\", \"rouge_L\"]) # Add headers\n",
    "    for i in range(50):\n",
    "        print(f\"\\n###Document: {i+1}###\\n\")\n",
    "        \n",
    "        # Generate summary\n",
    "        summary = summarize_chunks(X_train[i])\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        cosine_sim = cosine_similarity(tfidf.transform([y_train[i]]), tfidf.transform([summary]))\n",
    "        \n",
    "        # Compute ROUGE scores\n",
    "        rouge_1, rouge_2, rouge_L = rouge_scores(y_train[i], summary)\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"\\ncosine similarity: {cosine_sim}\")\n",
    "        print(f\"rouge_1: {rouge_1}\")\n",
    "        print(f\"rouge_2: {rouge_2}\")\n",
    "        print(f\"rouge_L: {rouge_L}\\n\")\n",
    "        \n",
    "        # Write to CSV\n",
    "        writer.writerow([X_train[i], y_train[i], summary, cosine_sim, rouge_1, rouge_2, rouge_L])\n",
    "        \n",
    "        # Clean up\n",
    "        del summary\n",
    "        del cosine_sim\n",
    "        del rouge_1\n",
    "        del rouge_2\n",
    "        del rouge_L\n",
    "        \n",
    "        # Clear GPU memory\n",
    "        clear_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388b6ddc-bf3c-4fa2-84cb-afef7e223b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"val_evaluation.csv\"\n",
    "\n",
    "# Open the CSV file and write the headers\n",
    "with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"document\", \"actual_summary\", \"predicted_summary\", \"cosine_similarity\", \"rouge_1\", \"rouge_2\", \"rouge_L\"])  # Add headers\n",
    "    \n",
    "    # Iterate over 20 documents\n",
    "    for i in tqdm(range(20), desc=\"Processing Documents\", leave=False):\n",
    "        print(f\"\\n### Document: {i+1} ###\\n\")\n",
    "        \n",
    "        # Generate summary for the document\n",
    "        summary = summarize_chunks(X_val[i])\n",
    "        \n",
    "        # Compute cosine similarity using TF-IDF vectors (ensure `tfidf` is already defined and fitted)\n",
    "        cosine_sim = cosine_similarity(tfidf.transform([y_val[i]]), tfidf.transform([summary]))[0][0]\n",
    "        \n",
    "        # Compute ROUGE scores (assuming rouge_scores is a predefined function)\n",
    "        rouge_1, rouge_2, rouge_L = rouge_scores(y_val[i], summary)\n",
    "        \n",
    "        # Print metrics for inspection\n",
    "        print(f\"\\nCosine similarity: {cosine_sim}\")\n",
    "        print(f\"ROUGE-1: {rouge_1}\")\n",
    "        print(f\"ROUGE-2: {rouge_2}\")\n",
    "        print(f\"ROUGE-L: {rouge_L}\\n\")\n",
    "        \n",
    "        # Write the data to CSV file\n",
    "        writer.writerow([X_val[i], y_val[i], summary, cosine_sim, rouge_1, rouge_2, rouge_L])\n",
    "        \n",
    "        # Clear memory for unused variables\n",
    "        del summary\n",
    "        del cosine_sim\n",
    "        del rouge_1\n",
    "        del rouge_2\n",
    "        del rouge_L\n",
    "        \n",
    "        # Explicitly clear GPU memory and trigger garbage collection\n",
    "        clear_gpu_memory()\n",
    "\n",
    "print(\"Processing and evaluation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a63fac-10e8-43a8-93b8-3e4404d2995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BART performance on Training data\n",
    "train_bart = pd.read_csv(\"train_evaluation.csv\")\n",
    "train_bart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9c3cea-3399-4fc2-8ce5-24bef28f02f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bart[\"cosine_similarity_\"] = train_bart[\"cosine_similarity\"].astype(str)\n",
    "\n",
    "# Define a lambda function to remove square brackets\n",
    "remove_brackets = lambda x: x.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "\n",
    "train_bart['cosine_similarity_'] = train_bart['cosine_similarity_'].apply(remove_brackets).astype(\"float64\")\n",
    "train_bart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e20ef3-e9b1-41b7-a964-59c2ec66a561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots for histograms with KDE for Cosine Similarities and ROUGE Scores\n",
    "cosine_similarity_scores = train_bart[\"cosine_similarity_\"]\n",
    "rouge_1_scores = train_bart[\"rouge_1\"]\n",
    "rouge_2_scores = train_bart[\"rouge_2\"]\n",
    "rouge_L_scores = train_bart[\"rouge_L\"]\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Cosine Similarity subplot\n",
    "sns.histplot(cosine_similarity_scores, kde=True, bins=10, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Cosine Similarity Scores')\n",
    "axes[0, 0].set_xlabel('Cosine Similarity')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# ROUGE-1 subplot\n",
    "sns.histplot(rouge_1_scores, kde=True, bins=10, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('ROUGE-1 Scores')\n",
    "axes[0, 1].set_xlabel('ROUGE-1')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# ROUGE-2 subplot\n",
    "sns.histplot(rouge_2_scores, kde=True, bins=10, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('ROUGE-2 Scores')\n",
    "axes[1, 0].set_xlabel('ROUGE-2')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# ROUGE-L subplot\n",
    "sns.histplot(rouge_L_scores, kde=True, bins=10, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('ROUGE-L Scores')\n",
    "axes[1, 1].set_xlabel('ROUGE-L')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "plt.suptitle('Training Data')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10135c0-8ccc-40b1-9167-584b8fe89352",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rouge_1 = train_bart[\"rouge_1\"].mean()\n",
    "avg_rouge_2 = train_bart[\"rouge_2\"].mean()\n",
    "avg_rouge_L = train_bart[\"rouge_L\"].mean()\n",
    "\n",
    "rouge_df = pd.DataFrame([[avg_rouge_1, avg_rouge_2, avg_rouge_L]], columns=[\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\"], index=[\"BART\"])\n",
    "rouge_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
